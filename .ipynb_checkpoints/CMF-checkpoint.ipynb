{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn import linear_model\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import scipy.io\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "Character = pd.read_csv('Data/AAPL_char.csv')\n",
    "Forecast = pd.read_csv('Data/AAPL_forecast.csv')\n",
    "\n",
    "Character = Character.drop('firmID', axis=1).set_index('yq')\n",
    "\n",
    "Data_Y = Forecast[[ 'yq','firmID', 'actual']]\n",
    "Data_Y = Data_Y.groupby(['yq', 'firmID']).first().reset_index()\n",
    "Data_Y.drop('firmID', axis=1, inplace =True)\n",
    "Data_Y.set_index('yq', inplace = True)\n",
    "\n",
    "Data_X = Forecast[[ 'yq','firmID','analystID','value']]\n",
    "Data_X.drop('firmID', axis=1, inplace =True)\n",
    "\n",
    "Data_X =  Data_X.set_index(['yq', 'analystID']).unstack()\n",
    "Data_X.columns = list(Data_X.columns.droplevel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Functions and CMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(ratings):\n",
    "    test = np.zeros(ratings.shape)\n",
    "    test[:] = np.nan\n",
    "    train = ratings.copy()\n",
    "    for user in range(ratings.shape[0]):\n",
    "        test_ratings = np.random.choice(ratings[user, :].nonzero()[0], \n",
    "                                        #size=10, \n",
    "                                        (np.int((ratings.shape[1])*0.1)),\n",
    "                                        replace=False)\n",
    "        train[user, test_ratings] = np.nan\n",
    "        test[user, test_ratings] = ratings[user, test_ratings]\n",
    "        \n",
    "    # Test and training are truly disjoint\n",
    "    assert((np.isnan(train * test).all() == 1))\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    #pred = pred[actual.nonzero()].flatten()\n",
    "    pred = pred[~np.isnan(actual)].flatten()\n",
    "    #actual = actual[actual.nonzero()].flatten()\n",
    "    actual = actual[~np.isnan(actual)].flatten()\n",
    "    return mean_squared_error(pred, actual)\n",
    "\n",
    "\n",
    "\n",
    "def plot_learning_curve(iter_array, model):\n",
    "    plt.plot(iter_array, model.train_mse, \\\n",
    "             label='Training', linewidth=5)\n",
    "    plt.plot(iter_array, model.test_mse, \\\n",
    "             label='Test', linewidth=5)\n",
    "\n",
    "\n",
    "    plt.xticks(fontsize=16);\n",
    "    plt.yticks(fontsize=16);\n",
    "    plt.xlabel('iterations', fontsize=30);\n",
    "    plt.ylabel('MSE', fontsize=30);\n",
    "    plt.legend(loc='best', fontsize=20);\n",
    "    \n",
    "\n",
    "    \n",
    "def create_W_matrix(Data):\n",
    "    # create the 0,1 matrix for missing and non missing values\n",
    "    X = Data.copy()\n",
    "    for i in range (0, X.shape[0]):\n",
    "        for j in range (0, X.shape[1]):\n",
    "            if np.isnan(X[i,j]) == True:\n",
    "                X[i,j] = 0\n",
    "            else:\n",
    "                X[i,j] = 1\n",
    "    return X\n",
    "\n",
    "def MSE(original, imputed):\n",
    "    # calculate Mean Squared Error\n",
    "    return np.square(original-imputed).mean().mean()    \n",
    "\n",
    "def MPE(y_true, y_pred, threshold=0.005):\n",
    "    v = np.copy(y_true)\n",
    "    np.place(v, v==0, threshold)\n",
    "    #v = np.clip(np.abs(y_true), threshold, None)\n",
    "    diff = np.abs((y_true - y_pred) / v)\n",
    "    return np.mean(diff, axis=-1).mean()\n",
    "\n",
    "\n",
    "def R_squared(original, predicted):\n",
    "    Differ = np.square(original-predicted)\n",
    "    m = np.mean(original) \n",
    "    denom = np.square(original - m)\n",
    "    R_sq = 1 - ((Differ.sum())/denom.sum())\n",
    "    return R_sq\n",
    "\n",
    "\n",
    "def Get_performance(Y_true, Y_pred): \n",
    "    #print (\"R2 : \", np.round(R_squared(Y_true, Y_pred), 4))\n",
    "    #print (\"MSE\",  np.round(MSE(Y_true, Y_pred), 4))\n",
    "    #print (\"MPE\", np.round(MPE(Y_true, Y_pred), 4))\n",
    "    return (np.round(R_squared(Y_true, Y_pred), 4), np.round(MSE(Y_true, Y_pred), 4), np.round(MPE(Y_true, Y_pred), 4)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplicitMF():\n",
    "    def __init__(self, \n",
    "                 ratings,\n",
    "                 n_factors=40,\n",
    "                 learning='sgd',\n",
    "                 item_fact_reg=0.0, \n",
    "                 user_fact_reg=0.0,\n",
    "                 item_bias_reg=0.0,\n",
    "                 user_bias_reg=0.0,\n",
    "                 verbose=False):\n",
    "        \"\"\"\n",
    "        Train a matrix factorization model to predict empty \n",
    "        entries in a matrix. The terminology assumes a \n",
    "        ratings matrix which is ~ user x item\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        ratings : (ndarray)\n",
    "            User x Item matrix with corresponding ratings\n",
    "        \n",
    "        n_factors : (int)\n",
    "            Number of latent factors to use in matrix \n",
    "            factorization model\n",
    "        learning : (str)\n",
    "            Method of optimization. Options include \n",
    "            'sgd' or 'als'.\n",
    "        \n",
    "        item_fact_reg : (float)\n",
    "            Regularization term for item latent factors\n",
    "        \n",
    "        user_fact_reg : (float)\n",
    "            Regularization term for user latent factors\n",
    "            \n",
    "        item_bias_reg : (float)\n",
    "            Regularization term for item biases\n",
    "        \n",
    "        user_bias_reg : (float)\n",
    "            Regularization term for user biases\n",
    "        \n",
    "        verbose : (bool)\n",
    "            Whether or not to printout training progress\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ratings = ratings\n",
    "        self.n_users, self.n_items = ratings.shape\n",
    "        self.n_factors = n_factors\n",
    "        self.item_fact_reg = item_fact_reg\n",
    "        self.user_fact_reg = user_fact_reg\n",
    "        self.item_bias_reg = item_bias_reg\n",
    "        self.user_bias_reg = user_bias_reg\n",
    "        self.learning = learning\n",
    "        if self.learning == 'sgd':\n",
    "            #self.sample_row, self.sample_col = self.ratings.nonzero()\n",
    "            self.sample_row = np.argwhere(~np.isnan(self.ratings))[:,0]\n",
    "            self.sample_col = np.argwhere(~np.isnan(self.ratings))[:,1]\n",
    "            \n",
    "            self.n_samples = len(self.sample_row)\n",
    "        self._v = verbose\n",
    "\n",
    "    def als_step(self,\n",
    "                 latent_vectors,\n",
    "                 fixed_vecs,\n",
    "                 ratings,\n",
    "                 _lambda,\n",
    "                 type='user'):\n",
    "        \"\"\"\n",
    "        One of the two ALS steps. Solve for the latent vectors\n",
    "        specified by type.\n",
    "        \"\"\"\n",
    "        if type == 'user':\n",
    "            # Precompute\n",
    "            YTY = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(YTY.shape[0]) * _lambda\n",
    "\n",
    "            for u in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[u, :] = solve((YTY + lambdaI), \n",
    "                                             ratings[u, :].dot(fixed_vecs))\n",
    "        elif type == 'item':\n",
    "            # Precompute\n",
    "            XTX = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(XTX.shape[0]) * _lambda\n",
    "            \n",
    "            for i in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[i, :] = solve((XTX + lambdaI), \n",
    "                                             ratings[:, i].T.dot(fixed_vecs))\n",
    "        return latent_vectors\n",
    "\n",
    "    def train(self, n_iter=10, learning_rate=0.1):\n",
    "        \"\"\" Train model for n_iter iterations from scratch.\"\"\"\n",
    "        # initialize latent vectors        \n",
    "        self.user_vecs = np.random.normal(scale=1./self.n_factors,\\\n",
    "                                          size=(self.n_users, self.n_factors))\n",
    "        self.item_vecs = np.random.normal(scale=1./self.n_factors,\n",
    "                                          size=(self.n_items, self.n_factors))\n",
    "        \n",
    "        if self.learning == 'als':\n",
    "            self.partial_train(n_iter)\n",
    "        elif self.learning == 'sgd':\n",
    "            self.learning_rate = learning_rate\n",
    "            self.user_bias = np.zeros(self.n_users)\n",
    "            self.item_bias = np.zeros(self.n_items)\n",
    "            #self.global_bias = np.mean(self.ratings[np.where(self.ratings != 0)])\n",
    "            self.global_bias = np.mean(self.ratings[~np.isnan(self.ratings)])\n",
    "            self.partial_train(n_iter)\n",
    "    \n",
    "    \n",
    "    def partial_train(self, n_iter):\n",
    "        \"\"\" \n",
    "        Train model for n_iter iterations. Can be \n",
    "        called multiple times for further training.\n",
    "        \"\"\"\n",
    "        ctr = 1\n",
    "        while ctr <= n_iter:\n",
    "            if ctr % 10 == 0 and self._v:\n",
    "                print ('\\tcurrent iteration: {}'.format(ctr))\n",
    "            if self.learning == 'als':\n",
    "                self.user_vecs = self.als_step(self.user_vecs, \n",
    "                                               self.item_vecs, \n",
    "                                               self.ratings, \n",
    "                                               self.user_fact_reg, \n",
    "                                               type='user')\n",
    "                self.item_vecs = self.als_step(self.item_vecs, \n",
    "                                               self.user_vecs, \n",
    "                                               self.ratings, \n",
    "                                               self.item_fact_reg, \n",
    "                                               type='item')\n",
    "            elif self.learning == 'sgd':\n",
    "                self.training_indices = np.arange(self.n_samples)\n",
    "                np.random.shuffle(self.training_indices)\n",
    "                self.sgd()\n",
    "            ctr += 1\n",
    "\n",
    "    def sgd(self):\n",
    "        for idx in self.training_indices:\n",
    "            u = self.sample_row[idx]\n",
    "            i = self.sample_col[idx]\n",
    "            prediction = self.predict(u, i)\n",
    "            e = (self.ratings[u,i] - prediction) # error\n",
    "            \n",
    "            \n",
    "            # Update biases        \n",
    "            self.user_bias[u] += self.learning_rate * \\\n",
    "                                (e - self.user_bias_reg * self.user_bias[u])\n",
    "            \n",
    "    \n",
    "            self.item_bias[i] += self.learning_rate * (e - self.item_bias_reg * self.item_bias[i])\n",
    "\n",
    "            \n",
    "            #Update latent factors\n",
    "            self.user_vecs[u, :] += self.learning_rate * \\\n",
    "                                    (e * self.item_vecs[i, :] - \\\n",
    "                                     self.user_fact_reg * self.user_vecs[u,:])\n",
    "            self.item_vecs[i, :] += self.learning_rate * \\\n",
    "                                    (e * self.user_vecs[u, :] - \\\n",
    "                                     self.item_fact_reg * self.item_vecs[i,:])\n",
    "    def predict(self, u, i):\n",
    "        \"\"\" Single user and item prediction.\"\"\"\n",
    "        if self.learning == 'als':\n",
    "            return self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "        elif self.learning == 'sgd':\n",
    "            prediction = self.global_bias + self.user_bias[u] + self.item_bias[i]\n",
    "            prediction += self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "            return prediction\n",
    "    \n",
    "    def predict_all(self):\n",
    "        \"\"\" Predict ratings for every user and item.\"\"\"\n",
    "        predictions = np.zeros((self.user_vecs.shape[0], \n",
    "                                self.item_vecs.shape[0]))\n",
    "        for u in range(self.user_vecs.shape[0]):\n",
    "            for i in range(self.item_vecs.shape[0]):\n",
    "                predictions[u, i] = self.predict(u, i)\n",
    "                \n",
    "        return predictions\n",
    "    \n",
    "    def calculate_learning_curve(self, iter_array, test, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Keep track of MSE as a function of training iterations.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        iter_array : (list)\n",
    "            List of numbers of iterations to train for each step of \n",
    "            the learning curve. e.g. [1, 5, 10, 20]\n",
    "        test : (2D ndarray)\n",
    "            Testing dataset (assumed to be user x item).\n",
    "        \n",
    "        The function creates two new class attributes:\n",
    "        \n",
    "        train_mse : (list)\n",
    "            Training data MSE values for each value of iter_array\n",
    "        test_mse : (list)\n",
    "            Test data MSE values for each value of iter_array\n",
    "        \"\"\"\n",
    "        iter_array.sort()\n",
    "        self.train_mse =[]\n",
    "        self.test_mse = []\n",
    "        iter_diff = 0\n",
    "        for (i, n_iter) in enumerate(iter_array):\n",
    "            if self._v:\n",
    "                print ('Iteration: {}'.format(n_iter))\n",
    "            if i == 0:\n",
    "                self.train(n_iter - iter_diff, learning_rate)\n",
    "            else:\n",
    "                self.partial_train(n_iter - iter_diff)\n",
    "\n",
    "            predictions = self.predict_all()\n",
    "            \n",
    "            self.train_mse += [get_mse(predictions, self.ratings)]\n",
    "            self.test_mse += [get_mse(predictions, test)]\n",
    "            if self._v:\n",
    "                print ('Train mse: ' + str(self.train_mse[-1]))\n",
    "                print ('Test mse: ' + str(self.test_mse[-1]))\n",
    "            iter_diff = n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class secondMF():\n",
    "    def __init__(self, \n",
    "                 ratings,\n",
    "                 user_vecs,\n",
    "                 n_factors=40,\n",
    "                 learning='sgd',\n",
    "                 item_fact_reg=0.0, \n",
    "                 user_fact_reg=0.0,\n",
    "                 item_bias_reg=0.0,\n",
    "                 user_bias_reg=0.0,\n",
    "                 verbose=False):\n",
    "        \"\"\"\n",
    "        Train a matrix factorization model to predict empty \n",
    "        entries in a matrix. The terminology assumes a \n",
    "        ratings matrix which is ~ user x item\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        ratings : (ndarray)\n",
    "            User x Item matrix with corresponding ratings\n",
    "        \n",
    "        n_factors : (int)\n",
    "            Number of latent factors to use in matrix \n",
    "            factorization model\n",
    "        learning : (str)\n",
    "            Method of optimization. Options include \n",
    "            'sgd' or 'als'.\n",
    "        \n",
    "        item_fact_reg : (float)\n",
    "            Regularization term for item latent factors\n",
    "        \n",
    "        user_fact_reg : (float)\n",
    "            Regularization term for user latent factors\n",
    "            \n",
    "        item_bias_reg : (float)\n",
    "            Regularization term for item biases\n",
    "        \n",
    "        user_bias_reg : (float)\n",
    "            Regularization term for user biases\n",
    "        \n",
    "        verbose : (bool)\n",
    "            Whether or not to printout training progress\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ratings = ratings\n",
    "        self.user_vecs = user_vecs\n",
    "        self.n_users, self.n_items = ratings.shape\n",
    "        self.n_factors = n_factors\n",
    "        self.item_fact_reg = item_fact_reg\n",
    "        self.user_fact_reg = user_fact_reg\n",
    "        self.item_bias_reg = item_bias_reg\n",
    "        self.user_bias_reg = user_bias_reg\n",
    "        self.learning = learning\n",
    "        if self.learning == 'sgd':\n",
    "            #self.sample_row, self.sample_col = self.ratings.nonzero()\n",
    "            \n",
    "            self.sample_row = np.argwhere(~np.isnan(self.ratings))[:,0]\n",
    "            self.sample_col = np.argwhere(~np.isnan(self.ratings))[:,1]\n",
    "            \n",
    "            self.n_samples = len(self.sample_row)\n",
    "        self._v = verbose\n",
    "\n",
    "    def als_step(self,\n",
    "                 latent_vectors,\n",
    "                 fixed_vecs,\n",
    "                 ratings,\n",
    "                 _lambda,\n",
    "                 type='user'):\n",
    "        \"\"\"\n",
    "        One of the two ALS steps. Solve for the latent vectors\n",
    "        specified by type.\n",
    "        \"\"\"\n",
    "        if type == 'user':\n",
    "            # Precompute\n",
    "            YTY = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(YTY.shape[0]) * _lambda\n",
    "\n",
    "            for u in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[u, :] = solve((YTY + lambdaI), \n",
    "                                             ratings[u, :].dot(fixed_vecs))\n",
    "        elif type == 'item':\n",
    "            # Precompute\n",
    "            XTX = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(XTX.shape[0]) * _lambda\n",
    "            \n",
    "            for i in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[i, :] = solve((XTX + lambdaI), \n",
    "                                             ratings[:, i].T.dot(fixed_vecs))\n",
    "        return latent_vectors\n",
    "\n",
    "    def train(self, n_iter=10, learning_rate=0.1):\n",
    "        \"\"\" Train model for n_iter iterations from scratch.\"\"\"\n",
    "        # initialize latent vectors        \n",
    "        self.item_vecs = np.random.normal(scale=1./self.n_factors,\n",
    "                                          size=(self.n_items, self.n_factors))\n",
    "        \n",
    "        if self.learning == 'als':\n",
    "            self.partial_train(n_iter)\n",
    "        elif self.learning == 'sgd':\n",
    "            self.learning_rate = learning_rate\n",
    "            self.user_bias = np.zeros(self.n_users)\n",
    "            self.item_bias = np.zeros(self.n_items)\n",
    "            #self.global_bias = np.mean(self.ratings[np.where(self.ratings != 0)])\n",
    "            self.global_bias = np.mean(self.ratings[~np.isnan(self.ratings)])\n",
    "            self.partial_train(n_iter)\n",
    "    \n",
    "    \n",
    "    def partial_train(self, n_iter):\n",
    "        \"\"\" \n",
    "        Train model for n_iter iterations. Can be \n",
    "        called multiple times for further training.\n",
    "        \"\"\"\n",
    "        ctr = 1\n",
    "        while ctr <= n_iter:\n",
    "            if ctr % 100 == 0 and self._v:\n",
    "                print ('\\tcurrent iteration: {}'.format(ctr))\n",
    "            if self.learning == 'als':\n",
    "                self.user_vecs = self.als_step(self.user_vecs, \n",
    "                                               self.item_vecs, \n",
    "                                               self.ratings, \n",
    "                                               self.user_fact_reg, \n",
    "                                               type='user')\n",
    "                self.item_vecs = self.als_step(self.item_vecs, \n",
    "                                               self.user_vecs, \n",
    "                                               self.ratings, \n",
    "                                               self.item_fact_reg, \n",
    "                                               type='item')\n",
    "            elif self.learning == 'sgd':\n",
    "                self.training_indices = np.arange(self.n_samples)\n",
    "                np.random.shuffle(self.training_indices)\n",
    "                self.sgd()\n",
    "            ctr += 1\n",
    "\n",
    "    def sgd(self):\n",
    "        for idx in self.training_indices:\n",
    "            u = self.sample_row[idx]\n",
    "            i = self.sample_col[idx]\n",
    "            prediction = self.predict(u, i)\n",
    "            e = (self.ratings[u,i] - prediction) # error\n",
    "            \n",
    "            # Update biases\n",
    "            self.user_bias[u] += self.learning_rate * \\\n",
    "                                (e - self.user_bias_reg * self.user_bias[u])\n",
    "            self.item_bias[i] += self.learning_rate * \\\n",
    "                                (e - self.item_bias_reg * self.item_bias[i])\n",
    "            \n",
    "            #Update latent factors\n",
    "            self.user_vecs[u, :] += self.learning_rate * \\\n",
    "                                    (e * self.item_vecs[i, :] - \\\n",
    "                                     self.user_fact_reg * self.user_vecs[u,:])\n",
    "            self.item_vecs[i, :] += self.learning_rate * \\\n",
    "                                    (e * self.user_vecs[u, :] - \\\n",
    "                                     self.item_fact_reg * self.item_vecs[i,:])\n",
    "    def predict(self, u, i):\n",
    "        \"\"\" Single user and item prediction.\"\"\"\n",
    "        if self.learning == 'als':\n",
    "            return self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "        elif self.learning == 'sgd':\n",
    "            prediction = self.global_bias + self.user_bias[u] + self.item_bias[i]\n",
    "            prediction += self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "            return prediction\n",
    "    \n",
    "    def predict_all(self):\n",
    "        \"\"\" Predict ratings for every user and item.\"\"\"\n",
    "        predictions = np.zeros((self.user_vecs.shape[0], \n",
    "                                self.item_vecs.shape[0]))\n",
    "        for u in range(self.user_vecs.shape[0]):\n",
    "            for i in range(self.item_vecs.shape[0]):\n",
    "                predictions[u, i] = self.predict(u, i)\n",
    "                \n",
    "        return predictions\n",
    "    \n",
    "    def calculate_learning_curve(self, iter_array, test, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Keep track of MSE as a function of training iterations.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        iter_array : (list)\n",
    "            List of numbers of iterations to train for each step of \n",
    "            the learning curve. e.g. [1, 5, 10, 20]\n",
    "        test : (2D ndarray)\n",
    "            Testing dataset (assumed to be user x item).\n",
    "        \n",
    "        The function creates two new class attributes:\n",
    "        \n",
    "        train_mse : (list)\n",
    "            Training data MSE values for each value of iter_array\n",
    "        test_mse : (list)\n",
    "            Test data MSE values for each value of iter_array\n",
    "        \"\"\"\n",
    "        iter_array.sort()\n",
    "        self.train_mse =[]\n",
    "        self.test_mse = []\n",
    "        iter_diff = 0\n",
    "        for (i, n_iter) in enumerate(iter_array):\n",
    "            if self._v:\n",
    "                print ('Iteration: {}'.format(n_iter))\n",
    "            if i == 0:\n",
    "                self.train(n_iter - iter_diff, learning_rate)\n",
    "            else:\n",
    "                self.partial_train(n_iter - iter_diff)\n",
    "\n",
    "            predictions = self.predict_all()\n",
    "\n",
    "            self.train_mse += [get_mse(predictions, self.ratings)]\n",
    "            self.test_mse += [get_mse(predictions, test)]\n",
    "            if self._v:\n",
    "                print ('Train mse: ' + str(self.train_mse[-1]))\n",
    "                print ('Test mse: ' + str(self.test_mse[-1]))\n",
    "            iter_diff = n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(train, test, n_factors= 5, user_vectors = 0, type = 'first',):\n",
    "    iter_array = [1, 2, 100, 300, 500, 700, 1000, 1200]\n",
    "    learning_rates = [1e-5, 1e-4, 1e-3, 1e-2]\n",
    "    regularizations = [0.0001, 0.001, 0.01, 0.1]\n",
    "    #learning_rates = [1e-5, 1e-4 ]\n",
    "    #regularizations = [1e-5, 1e-4]\n",
    "    \n",
    "    regularizations.sort()\n",
    "\n",
    "    best_params = {}\n",
    "    best_params['learning_rate'] = None\n",
    "    best_params['reg'] = regularizations[0]\n",
    "    best_params['n_iter'] = 0\n",
    "    best_params['train_mse'] = np.inf\n",
    "    best_params['test_mse'] = np.inf\n",
    "    best_params['model'] = None\n",
    "\n",
    "    for rate in learning_rates:\n",
    "        print ('Rate: {}'.format(rate))\n",
    "        for reg in regularizations:\n",
    "            print ('Regularization: {}'.format(reg))\n",
    "            \n",
    "            if type == 'first':\n",
    "                MF_SGD = ExplicitMF(train, n_factors, learning='sgd',\\\n",
    "                                user_fact_reg=reg, item_fact_reg=reg, \\\n",
    "                                user_bias_reg=reg, item_bias_reg=reg)\n",
    "    \n",
    "                \n",
    "            elif type == 'second':  \n",
    "                MF_SGD = secondMF(train, user_vectors, n_factors, learning='sgd',\\\n",
    "                                user_fact_reg=reg, item_fact_reg=reg, \\\n",
    "                                user_bias_reg=reg, item_bias_reg=reg)\n",
    "                \n",
    "                \n",
    "                \n",
    "            MF_SGD.calculate_learning_curve(iter_array, test, learning_rate= rate)\n",
    "            min_idx = np.argmin(MF_SGD.test_mse)\n",
    "            if MF_SGD.test_mse[min_idx] < best_params['test_mse']:\n",
    "                best_params['learning_rate'] = rate\n",
    "                best_params['reg'] = reg\n",
    "                best_params['n_iter'] = iter_array[min_idx]\n",
    "                best_params['train_mse'] = MF_SGD.train_mse[min_idx]\n",
    "                best_params['test_mse'] = MF_SGD.test_mse[min_idx]\n",
    "                best_params['model'] = MF_SGD\n",
    "    print ('New optimal hyperparameters')\n",
    "    print (pd.Series(best_params))\n",
    "    return best_params\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grid_SVR(X_train, Y_train, X_test, Y_test):\n",
    "    epsilon = [1e-5, 1e-4]\n",
    "    Con = [1, 2]\n",
    "    #epsilon = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1 ]\n",
    "    #Con = [1, 2, 3, 5, 10]\n",
    "    test_dif = []\n",
    "    best_params = {}\n",
    "    best_params['epsilon'] = None\n",
    "    best_params['train_result'] = np.inf\n",
    "    best_params['Con'] = None\n",
    "    best_params['Y'] = None\n",
    "    best_params['test_diff'] = np.inf\n",
    "    best_params['model'] = None\n",
    "    \n",
    "    for eps in epsilon:\n",
    "        for C in Con:\n",
    "            clf = SVR(gamma='scale',  C=C, epsilon=eps)\n",
    "            clf.fit(X_train, Y_train) \n",
    "            y_tr_pred = clf.predict(X_train)\n",
    "            y_ts_pred = clf.predict(X_test)\n",
    "        \n",
    "            \n",
    "            #print(eps, C, Get_performance(Y_train, y_tr_pred), y_ts_pred)\n",
    "            \n",
    "            if  (abs(Y_test - y_ts_pred)) < best_params['test_diff']:\n",
    "                best_params['train_result'] = Get_performance(Y_train, y_tr_pred)\n",
    "                best_params['epsilon'] = eps\n",
    "                best_params['Con'] = C\n",
    "                best_params['Y'] = y_ts_pred\n",
    "                best_params['model'] = clf \n",
    "                best_params['test_diff'] = abs(Y_test - y_ts_pred)\n",
    "            \n",
    "    return  best_params\n",
    "\n",
    "def Grid_LASSO(X_train, Y_train, X_test, Y_test):\n",
    "    #alpha = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    alpha = [1e-5, 1e-4]\n",
    "    best_params = {}\n",
    "\n",
    "    best_params['train_result'] = np.inf\n",
    "    best_params['alpha'] = None\n",
    "    best_params['Y'] = None\n",
    "    best_params['test_diff'] = np.inf\n",
    "    best_params['model'] = None\n",
    "    \n",
    "    for a in alpha:\n",
    "        Model_lasso = linear_model.Lasso(alpha=a, max_iter=3000)\n",
    "        Model_lasso.fit(X_train, Y_train)\n",
    "        y_tr_pred = Model_lasso.predict(X_train)\n",
    "        y_ts_pred = Model_lasso.predict(X_test)     \n",
    "            \n",
    "        if  (abs(Y_test - y_ts_pred)) < best_params['test_diff']:\n",
    "            best_params['train_result'] = Get_performance(Y_train, y_tr_pred)\n",
    "            best_params['alpha'] = a\n",
    "            best_params['Y'] = y_ts_pred\n",
    "            best_params['model'] = Model_lasso\n",
    "            best_params['test_diff'] = abs(Y_test - y_ts_pred)\n",
    "            \n",
    "    return  best_params\n",
    "\n",
    "\n",
    "\n",
    "def Grid_XGB(X_train, Y_train, X_test, Y_test):\n",
    "    learning_rate = [1e-4, 1e-3]\n",
    "    alpha = [1e-3]\n",
    "    #learning_rate = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    #alpha = [1e-3, 1e-2, 1e-1]\n",
    "    test_dif = []\n",
    "    best_params = {}\n",
    "    best_params['learning_rate'] = None\n",
    "    best_params['train_result'] = np.inf\n",
    "    best_params['alpha'] = None\n",
    "    best_params['Y'] = None\n",
    "    best_params['test_diff'] = np.inf\n",
    "    best_params['model'] = None\n",
    "    \n",
    "    for l in learning_rate:\n",
    "        for a in alpha:\n",
    "            model_xgb = XGBRegressor(silent=False, learning_rate=l, n_estimators=100, \n",
    "                                     reg_alpha = a)\n",
    "            model_xgb.fit(X_train, Y_train)\n",
    "            y_tr_pred = model_xgb.predict(X_train)\n",
    "            y_ts_pred = model_xgb.predict(X_test)\n",
    "            \n",
    "            #print(l, a, Get_performance(Y_train, y_tr_pred), y_ts_pred)\n",
    "            \n",
    "            if  (abs(Y_test - y_ts_pred)) < best_params['test_diff']:\n",
    "                best_params['train_result'] = Get_performance(Y_train, y_tr_pred)\n",
    "                best_params['learning_rate'] = l\n",
    "                best_params['alpha'] = a\n",
    "                best_params['Y'] = y_ts_pred\n",
    "                best_params['model'] = model_xgb\n",
    "                best_params['test_diff'] = abs(Y_test - y_ts_pred)\n",
    "            \n",
    "    return  best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201501\n",
      "Rate: 1e-05\n",
      "Regularization: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization: 0.001\n",
      "Regularization: 0.01\n",
      "Regularization: 0.1\n",
      "Rate: 0.0001\n",
      "Regularization: 0.0001\n",
      "Regularization: 0.001\n",
      "Regularization: 0.01\n",
      "Regularization: 0.1\n",
      "Rate: 0.001\n",
      "Regularization: 0.0001\n",
      "Regularization: 0.001\n",
      "Regularization: 0.01\n",
      "Regularization: 0.1\n",
      "Rate: 0.01\n",
      "Regularization: 0.0001\n",
      "Regularization: 0.001\n",
      "Regularization: 0.01\n",
      "Regularization: 0.1\n",
      "New optimal hyperparameters\n",
      "learning_rate                                          0.001\n",
      "reg                                                     0.01\n",
      "n_iter                                                   500\n",
      "train_mse                                           0.178475\n",
      "test_mse                                            0.560248\n",
      "model            <__main__.ExplicitMF object at 0x142ad2650>\n",
      "dtype: object\n",
      "Rate: 1e-05\n",
      "Regularization: 0.0001\n",
      "Regularization: 0.001\n",
      "Regularization: 0.01\n",
      "Regularization: 0.1\n",
      "Rate: 0.0001\n",
      "Regularization: 0.0001\n",
      "Regularization: 0.001\n",
      "Regularization: 0.01\n",
      "Regularization: 0.1\n",
      "Rate: 0.001\n",
      "Regularization: 0.0001\n",
      "Regularization: 0.001\n",
      "Regularization: 0.01\n",
      "Regularization: 0.1\n",
      "Rate: 0.01\n",
      "Regularization: 0.0001\n",
      "Regularization: 0.001\n",
      "Regularization: 0.01\n",
      "Regularization: 0.1\n",
      "New optimal hyperparameters\n",
      "learning_rate                                         0.01\n",
      "reg                                                   0.01\n",
      "n_iter                                                1200\n",
      "train_mse                                      0.000917988\n",
      "test_mse                                        0.00208895\n",
      "model            <__main__.secondMF object at 0x142c71710>\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.024053794742679428, tolerance: 0.0028994484557333337\n",
      "  positive)\n",
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.053431576689587305, tolerance: 0.0028994484557333337\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.024262210050557627, tolerance: 0.0032258935125901644\n",
      "  positive)\n",
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0553028120144694, tolerance: 0.0032258935125901644\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.023115171092878983, tolerance: 0.003395227851483872\n",
      "  positive)\n",
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.05652574015704787, tolerance: 0.003395227851483872\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.022903727696985624, tolerance: 0.003588323017714286\n",
      "  positive)\n",
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.05752054218754198, tolerance: 0.003588323017714286\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.029260798762257404, tolerance: 0.0043051428600000006\n",
      "  positive)\n",
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.06697455546208289, tolerance: 0.0043051428600000006\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0263795031372556, tolerance: 0.0044655435317538465\n",
      "  positive)\n",
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0577541901023261, tolerance: 0.0044655435317538465\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.026054583023292603, tolerance: 0.0045249579043636365\n",
      "  positive)\n",
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.056695220510249944, tolerance: 0.0045249579043636365\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.026935084554421403, tolerance: 0.004626431847940299\n",
      "  positive)\n",
      "/Users/ajimuddin/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.06455502129587418, tolerance: 0.004626431847940299\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "SVRtrain_result = {}\n",
    "SVRtest_result = {}\n",
    "XGBtrain_result = {}\n",
    "XGBtest_result = {}\n",
    "LASSOtrain_result = {}\n",
    "LASSOtest_result = {}\n",
    "mean_train = {}\n",
    "mean_test = {}\n",
    "MXGBtrain_result = {}\n",
    "MXGBtest_result = {}\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#for i in (201501, 201502):\n",
    "for i in (201501, 201502, 201503, 201504, 201601, 201602, 201603, 201604):   \n",
    "    print(i)\n",
    "        \n",
    "    F_C = Character.iloc[Character.index.get_level_values('yq') <= i]\n",
    "    pt = PowerTransformer(method='yeo-johnson')\n",
    "    F_power = pt.fit_transform(F_C)\n",
    "    #F_power = pd.DataFrame(F_power, index=F_C.index, columns=F_C.columns)\n",
    "        \n",
    "    M_data = Data_X.iloc[Data_X.index.get_level_values('yq') <= i]\n",
    "    M_data.dropna(axis='columns', how = 'all', inplace = True)\n",
    "        \n",
    "        \n",
    "    Analyst_train, Analyst_test = train_test_split(M_data.values)\n",
    "    Firm_train, Firm_test = train_test_split(F_power)\n",
    "        \n",
    "    if i == 201501:\n",
    "        best_par = grid_search(Firm_train, Firm_test, 5,  type = 'first')\n",
    "        BS_second = grid_search(Analyst_train, Analyst_test, 5, best_par['model'].user_vecs, type = 'second')\n",
    "        \n",
    "        \n",
    "    MF_firm = ExplicitMF(F_power, n_factors=5, learning='sgd', user_fact_reg=best_par['reg'], \n",
    "                         item_fact_reg=best_par['reg'], user_bias_reg=best_par['reg'], item_bias_reg=best_par['reg'])\n",
    "    MF_firm.train(best_par['n_iter'], best_par['learning_rate'])\n",
    "        \n",
    "        \n",
    "    X_user = MF_firm.user_vecs\n",
    "    \n",
    "    MF_Analyst = secondMF(M_data.values, X_user, n_factors=5, learning='sgd', user_fact_reg=BS_second['reg'], \n",
    "                          item_fact_reg=BS_second['reg'], user_bias_reg=BS_second['reg'], item_bias_reg=BS_second['reg'])\n",
    "    MF_Analyst.train(BS_second['n_iter'], BS_second['learning_rate'])\n",
    "\n",
    "        \n",
    "        \n",
    "    W = create_W_matrix(M_data.values)\n",
    "    X = MF_Analyst.predict_all()\n",
    "    X_imputed = (X * (1-W)) + np.nan_to_num(M_data.values)\n",
    "        \n",
    "        \n",
    "    X_train  =   X_imputed[0:-1,:]\n",
    "    X_test =   X_imputed[-1,:].reshape(1,-1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    Y_train = Data_Y.iloc[Data_Y.index.get_level_values('yq') < i].values.flatten()\n",
    "    Y_test =  Data_Y.iloc[Data_Y.index.get_level_values('yq') == i].values.flatten()\n",
    "        \n",
    "        \n",
    "        \n",
    "    Mod_SVR = Grid_SVR(X_train, Y_train, X_test, Y_test)\n",
    "    SVRtest_result[i] = Mod_SVR['Y']\n",
    "        \n",
    "    Mod_LASSO = Grid_LASSO(X_train, Y_train, X_test, Y_test)\n",
    "    LASSOtest_result[i] = Mod_LASSO['Y']\n",
    "        \n",
    "        \n",
    "    model_xgb = Grid_XGB(X_train, Y_train, X_test, Y_test)\n",
    "    XGBtest_result[i] = model_xgb['Y']\n",
    "\n",
    "        \n",
    "        \n",
    "    ori_X_tr =  M_data.iloc[M_data.index.get_level_values('yq') < i]\n",
    "    ori_X_ts =  M_data.iloc[M_data.index.get_level_values('yq') == i]\n",
    "        #mean_train[i] = Get_performance(Y_train, ori_X_tr.mean(axis=1).values)\n",
    "        \n",
    "        \n",
    "    mean_test[i] = ori_X_ts.mean(axis=1).values\n",
    "\n",
    "        \n",
    "    model_mxgb = Grid_XGB(ori_X_tr, Y_train, ori_X_ts, Y_test)\n",
    "    MXGBtest_result[i] = model_mxgb['Y']\n",
    "        \n",
    "        \n",
    "        \n",
    "#Test_d = Group_Y[ABC].iloc[ Group_Y[ABC].index.get_level_values('yq') >= 201501].values.flatten()\n",
    "Test_d = Data_Y.iloc[Data_Y.index.get_level_values('yq') >= 201501].values.flatten()\n",
    "    \n",
    "SVR_Test = pd.DataFrame.from_dict(SVRtest_result).values.flatten()\n",
    "XGB_Test = pd.DataFrame.from_dict(XGBtest_result).values.flatten()\n",
    "LASSO_Test = pd.DataFrame.from_dict(LASSOtest_result).values.flatten()\n",
    "M_Test = pd.DataFrame.from_dict(mean_test).values.flatten()\n",
    "MXGB_Test = pd.DataFrame.from_dict(MXGBtest_result).values.flatten()\n",
    "\n",
    "\n",
    "    \n",
    "a = pd.DataFrame(Get_performance(Test_d, M_Test), index=['R_2','MSE', 'MPE'], columns = ['Mean'])\n",
    "b = pd.DataFrame(Get_performance(Test_d, MXGB_Test), index=['R_2','MSE', 'MPE'], columns = ['MXGB'])\n",
    "c = pd.DataFrame(Get_performance(Test_d, LASSO_Test), index=['R_2','MSE', 'MPE'], columns = ['Lasso'])\n",
    "d = pd.DataFrame(Get_performance(Test_d,XGB_Test), index=['R_2','MSE', 'MPE'], columns = ['XGB'])\n",
    "e = pd.DataFrame(Get_performance(Test_d,SVR_Test), index=['R_2','MSE', 'MPE'], columns = ['SVR'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pd.merge(a,b, left_index=True, right_index = True)\n",
    "ss = pd.merge(ss,c, left_index=True, right_index = True)\n",
    "ss = pd.merge(ss,d, left_index=True, right_index = True)\n",
    "comparison_result = pd.merge(ss,e, left_index=True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Mean    MXGB   Lasso     XGB     SVR\n",
      "R_2  0.9776 -5.4354  0.9794 -5.4733  0.6338\n",
      "MSE  0.0103  2.9641  0.0095  2.9815  0.1687\n",
      "MPE  0.0371  0.6868  0.0368  0.6943  0.0668\n"
     ]
    }
   ],
   "source": [
    "print(comparison_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
